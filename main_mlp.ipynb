{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Utility Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "\n",
                "def linear(x):\n",
                "    return x\n",
                "\n",
                "\n",
                "def relu(x):\n",
                "    return np.maximum(x, 0)\n",
                "\n",
                "\n",
                "def sigmoid(x):\n",
                "    return 1.0 / (1.0 + np.exp(-x))\n",
                "\n",
                "\n",
                "def softmax(x):\n",
                "    e_x = np.exp(x - np.max(x))\n",
                "    return e_x / e_x.sum(axis=0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## MLPClassifier Implementation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Parameter yang ada pada kelas MLPClassifier yaitu: **struktur jaringan (jumlah layer, jumlah neuron setiap layer, fungsi aktivasi setiap layer), initial weights tiap neuron, learning_rate, error_threshold, max_iter, batch_size**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "import numpy as np\n",
                "import os\n",
                "import json\n",
                "\n",
                "\n",
                "class FFNNLayer:\n",
                "    def __init__(self, number_of_neurons: int, activation_function: str):\n",
                "        \"\"\"\n",
                "        :param number_of_neurons:\n",
                "        :param activation_function:\n",
                "        \"\"\"\n",
                "        self.number_of_neurons = number_of_neurons\n",
                "        self.activation_function = activation_function\n",
                "\n",
                "\n",
                "class MLPClassifier:\n",
                "    def __init__(self, layers: list, weights, learning_rate=None, error_threshold=None, max_iter=None, batch_size=None,  stopped_by=None, expected_weights = None):\n",
                "        \"\"\"\n",
                "        :param layers: list of FFNNLayer to specify the activation function and num of neurons for each layers\n",
                "        :param learning_rate: the learning rate\n",
                "        :param error_threshold: the error threshold\n",
                "        :param max_iter: max iter to stop iteration\n",
                "        :param batch_size: the size of batch for each mini batch\n",
                "        \"\"\"\n",
                "        self.num_of_layers = len(layers)\n",
                "        self.layers = layers\n",
                "        self.learning_rate = learning_rate\n",
                "        self.error_threshold = error_threshold\n",
                "        self.error_sum = 1\n",
                "        self.max_iter = max_iter\n",
                "        self.batch_size = batch_size\n",
                "        self.X_train = []\n",
                "        self.y_train = []\n",
                "        self.neuron_values = []\n",
                "        self.weights = [weight[1:] for weight in weights]\n",
                "        self.bias_weights = [weight[0] for weight in weights]\n",
                "        self.prediction = []\n",
                "        self.num_of_features = 0\n",
                "        self.num_of_batches = 0\n",
                "        self.d_weights = None\n",
                "        self.d_bias_weights = None\n",
                "        self.expected_stopped_by = stopped_by\n",
                "        self.expected_weights = expected_weights\n",
                "        self.expected_output = None\n",
                "        self.stopped_by = None\n",
                "        self.current_inputs = None\n",
                "\n",
                "    def fit(self, X_train, y_train):\n",
                "        self.X_train = X_train\n",
                "        self.y_train = y_train\n",
                "        self.num_of_features = len(self.X_train)\n",
                "        self.num_of_batches = math.ceil(len(self.X_train) / self.batch_size)\n",
                "\n",
                "        # the first neuron is the X inputs themselves\n",
                "        self.neuron_values = [[None for _ in range(layer.number_of_neurons)] for layer in self.layers]\n",
                "        num_iter = 0\n",
                "        while num_iter < self.max_iter:\n",
                "            num_of_batches = math.ceil(len(self.X_train) / self.batch_size)\n",
                "            err = 0\n",
                "            for i in range(num_of_batches):\n",
                "                self.__forward(i)\n",
                "                self.__backward(i)\n",
                "                err += self.__calculate_error(i)\n",
                "\n",
                "            # Update the average error for this iteration\n",
                "            self.error_sum = err / num_of_batches\n",
                "\n",
                "            # Check if the error is below the threshold\n",
                "            if self.error_sum <= self.error_threshold:\n",
                "                break\n",
                "\n",
                "            num_iter += 1\n",
                "\n",
                "        self.stopped_by = \"max_iteration\" if num_iter == self.max_iter else \"error_threshold\"\n",
                "\n",
                "        if self.expected_weights:\n",
                "            self.__print_final_weights()\n",
                "\n",
                "    def predict(self, X_test):\n",
                "        \"\"\"Perform forward pass to make predictions on input X_test\n",
                "\n",
                "        Args:\n",
                "            X_test: Input data for prediction (list)\n",
                "\n",
                "        Returns:\n",
                "            Predicted outputs for each sample in X_test\n",
                "        \"\"\"\n",
                "        current_inputs = np.array(X_test)\n",
                "        for i in range(self.num_of_layers):\n",
                "            net = [np.matmul(x, self.weights[i]) + self.bias_weights[i] for x in current_inputs]\n",
                "            act_func = self.layers[i].activation_function\n",
                "            if act_func == 'linear':\n",
                "                res = [linear(x) for x in net]\n",
                "            elif act_func == 'relu':\n",
                "                res = [relu(n) for n in net]\n",
                "            elif act_func == 'sigmoid':\n",
                "                res = [sigmoid(n) for n in net]\n",
                "            elif act_func == \"softmax\":\n",
                "                res = [softmax(n) for n in net]\n",
                "            current_inputs = res\n",
                "        return res\n",
                "\n",
                "    def calculate_sse(self):\n",
                "        sse = 0\n",
                "        for layer in range(len(self.expected_weights)):\n",
                "            for neuron in range(len(self.expected_weights[layer])):\n",
                "                expected = np.array(self.expected_weights[layer][neuron])\n",
                "                result = self.bias_weights[layer] if neuron == 0 else self.weights[layer][neuron-1]\n",
                "                squared_error = (expected - result) ** 2\n",
                "                sse += np.sum(squared_error)\n",
                "        return sse\n",
                "\n",
                "    def __forward(self, batch):\n",
                "        start_idx = self.batch_size * batch\n",
                "        self.expected_output = self.y_train[start_idx:start_idx + self.__get_curr_batch_size(batch)]\n",
                "        self.current_inputs = self.X_train[start_idx:start_idx + self.__get_curr_batch_size(batch)]\n",
                "        res = self.current_inputs\n",
                "        for i in range(self.num_of_layers):\n",
                "            net = [np.matmul(x, self.weights[i]) + self.bias_weights[i] for x in res]\n",
                "            act_func = self.layers[i].activation_function\n",
                "            if act_func == 'linear':\n",
                "                res = [linear(x) for x in net]\n",
                "            if act_func == 'relu':\n",
                "                res = [relu(n) for n in net]\n",
                "            if act_func == 'sigmoid':\n",
                "                res = [sigmoid(n) for n in net]\n",
                "            if act_func == \"softmax\":\n",
                "                res = [softmax(n) for n in net]\n",
                "            self.neuron_values[i] = res\n",
                "        # print(\"pred\", self.neuron_values[-1])    \n",
                "        self.prediction = list(self.neuron_values[-1])\n",
                "\n",
                "    def __backward(self, batch_idx):\n",
                "        \"\"\"\n",
                "        do backward propagation for each batch\n",
                "        :param batch_idx: the current batch that is processed\n",
                "        \"\"\"\n",
                "        self.__init_d_weights()\n",
                "        # get the current batch size\n",
                "        batch_size = self.__get_curr_batch_size(batch_idx)\n",
                "\n",
                "        # for each X in the batch\n",
                "        for i in range(batch_size):\n",
                "            d_k = np.zeros(0)\n",
                "            for j in range(self.num_of_layers - 1, -1, -1):\n",
                "                if j == self.num_of_layers - 1:       \n",
                "                    delta = self.__calc_output_layer_delta(i)\n",
                "                else:\n",
                "                    delta = self.__calc_hidden_layer_delta(i, j, d_k)\n",
                "                x = self.current_inputs[i] if j == 0 else self.neuron_values[j - 1][i]\n",
                "                self.d_weights[j] += np.array([[d * n for d in delta] for n in x])\n",
                "                self.d_bias_weights[j] += np.array(delta)\n",
                "                d_k = delta.reshape(delta.shape[0], 1)\n",
                "        \n",
                "        self.weights = [np.array(self.weights[k]) + np.array(self.d_weights[k]) * self.learning_rate for k in\n",
                "                        range(len(self.weights))]\n",
                "        self.bias_weights = [np.array(self.bias_weights[k]) + np.array(self.d_bias_weights[k]) * self.learning_rate for\n",
                "                             k in range(len(self.bias_weights))]\n",
                "\n",
                "    def __calculate_error(self, batch_idx):\n",
                "        \"\"\"\n",
                "        Calculate the error for the current batch\n",
                "        :param batch_idx: the current batch that is processed\n",
                "        \"\"\"\n",
                "        start_idx = self.batch_size * batch_idx\n",
                "        end_idx = start_idx + self.__get_curr_batch_size(batch_idx)\n",
                "        y_true = np.array(self.y_train[start_idx:end_idx])\n",
                "        y_pred = np.array(self.prediction)\n",
                "\n",
                "        # Get the activation function of the output layer\n",
                "        act_func = self.layers[-1].activation_function\n",
                "\n",
                "        # Calculate the error based on the activation function\n",
                "        if act_func in ['relu', 'sigmoid', 'linear']:\n",
                "            return 0.5 * np.sum((y_true - y_pred) ** 2)\n",
                "        elif act_func == 'softmax':\n",
                "            epsilon = 1e-20  # avoid taking log of zero\n",
                "            return -np.sum(y_true * np.log(y_pred + epsilon))\n",
                "        else:\n",
                "            raise ValueError(f\"Unsupported activation function: {act_func}\")\n",
                "\n",
                "    def __update_weights(self):\n",
                "        self.weights = [np.array(self.weights[k]) + np.array(self.d_weights[k]) * self.learning_rate for k in\n",
                "                        range(len(self.weights))]\n",
                "        self.bias_weights = [np.array(self.bias_weights[k]) + np.array(self.d_bias_weights[k]) * self.learning_rate for\n",
                "                             k in range(len(self.bias_weights))]\n",
                "\n",
                "    def __init_d_weights(self):\n",
                "        self.d_weights = [np.array([np.zeros(len(neuron_weight)) for neuron_weight in layer_weight])\n",
                "                          for layer_weight in self.weights]\n",
                "        self.d_bias_weights = [np.zeros(layer.number_of_neurons) for layer in self.layers]\n",
                "\n",
                "    def __calc_output_diff(self, x_idx: int) -> np.ndarray:\n",
                "        \"\"\"\n",
                "        :param x_idx:  the index of the current input on the X_train\n",
                "        \"\"\"\n",
                "        y_train = self.expected_output[x_idx]  # get the expected output of the x\n",
                "        output = self.prediction[x_idx]  # get the prediction\n",
                "        return np.array([y - p for y, p in zip(y_train, output)])\n",
                "\n",
                "    def __calc_act_function_derivative(self, act_func: str, y: list, target=None) -> np.ndarray:\n",
                "        \"\"\"\n",
                "        :param y:  y is the output in a layer\n",
                "\n",
                "        :return : a 1D array which is the sigmoid gradient of the neurons in a layer\n",
                "        \"\"\"\n",
                "        if act_func == 'sigmoid':\n",
                "            return np.array([x * (1-x) for x in y])\n",
                "\n",
                "        elif act_func == 'relu':\n",
                "            return np.array([1 if x > 0 else 0 for x in y])\n",
                "\n",
                "        elif act_func == 'linear':\n",
                "            return np.array([1 for _ in y])\n",
                "\n",
                "        elif act_func == 'softmax':\n",
                "            if target is None:\n",
                "                raise ValueError(\"Target is required for softmax gradient\")\n",
                "            return np.array([-1 * (1-y[i]) if target == i else y[i] for i in range(len(y))])\n",
                "\n",
                "        else:\n",
                "            raise ValueError(f\"Unknown activation function: {act_func}\")\n",
                "\n",
                "\n",
                "    def __calc_output_layer_delta(self, x_idx: int) -> np.ndarray:\n",
                "        \"\"\"\n",
                "        :param x_idx:  the index of the current input on the X_train\n",
                "        \"\"\"\n",
                "        # get the activation function for the last layer (output layer)\n",
                "        act_func = self.layers[-1].activation_function  # get the activation function\n",
                "    \n",
                "        if act_func == 'softmax':\n",
                "            return self.__calc_output_diff(x_idx)\n",
                "        return self.__calc_act_function_derivative(act_func, self.prediction[x_idx]) * self.__calc_output_diff(x_idx)\n",
                "\n",
                "    def __calc_hidden_layer_delta(self, batch_idx, layer_idx: int, output_error_term: np.ndarray) -> np.ndarray:\n",
                "        \"\"\"\n",
                "        :param output_error_term: a 1D array of the error term of each weight calculated from the layer after\n",
                "        :param layer_idx: the index of the current layer\n",
                "        :param batch_idx: the index of the current batch\n",
                "\n",
                "        hidden layer gradient = net gradient of the neuron values of current layer * the sum of weight * output error term\n",
                "        \"\"\"\n",
                "        act_func = self.layers[layer_idx].activation_function\n",
                "        activation_func_derivative = self.__calc_act_function_derivative(act_func,\n",
                "                                                                         self.neuron_values[layer_idx][batch_idx])\n",
                "\n",
                "        sum_d_net = [x[0] for x in np.matmul(self.weights[layer_idx + 1], output_error_term)]\n",
                "        return np.array(activation_func_derivative\n",
                "                        * sum_d_net)\n",
                "\n",
                "    def __get_curr_batch_size(self, batch_idx):\n",
                "        mod_res = len(self.X_train) % self.batch_size\n",
                "        if batch_idx == self.num_of_batches - 1 and mod_res != 0:\n",
                "            return mod_res\n",
                "        return self.batch_size\n",
                "\n",
                "    def __print_final_weights(self):\n",
                "        print(\"========= EXPECTED =========\")\n",
                "        for weight in self.expected_weights:\n",
                "            print(\"[\")\n",
                "            for neuron_weight in weight:\n",
                "                print(\"  \", neuron_weight)\n",
                "            print(\"], \")\n",
                "        print(\"STOPPED BY: \", self.expected_stopped_by)\n",
                "\n",
                "        print(\"========== ACTUAL ==========\")\n",
                "\n",
                "        for i in range(len(self.weights)):\n",
                "            print(\"[\")\n",
                "            print(\"  \", self.bias_weights[i])\n",
                "            for neuron_weight in self.weights[i]:\n",
                "                print(\"  \", neuron_weight)\n",
                "            print(\"], \")\n",
                "        print(\"STOPPED BY: \", self.stopped_by)\n",
                "    \n",
                "    def calc_score(self, y_true, predictions):\n",
                "        \"\"\"\n",
                "        Calculate the accuracy of predictions.\n",
                "\n",
                "        :param y_true: True labels.\n",
                "        :param predictions: Predictions from the model, as probabilities.\n",
                "        \n",
                "        :return: Accuracy as a float.\n",
                "        \"\"\"\n",
                "        y_pred_indices = np.argmax(predictions, axis=1)\n",
                "        y_true_indices = np.argmax(y_true, axis=1)\n",
                "        \n",
                "        accuracy = np.mean(y_pred_indices == y_true_indices)\n",
                "        return accuracy\n",
                "    \n",
                "    def save_model(self, file_name, directory=\"model\"):\n",
                "            \"\"\"\n",
                "            Saves the model weights and configuration to model directory.\n",
                "            \"\"\"\n",
                "            if not os.path.exists(directory):\n",
                "                os.makedirs(directory)\n",
                "            \n",
                "            model_data = {\n",
                "            \"final_weights\": [],\n",
                "            \"config\": {\n",
                "                \"layers\": [{\"number_of_neurons\": layer.number_of_neurons,\n",
                "                            \"activation_function\": layer.activation_function} for layer in self.layers],\n",
                "                }\n",
                "            }\n",
                "\n",
                "            for weights, bias in zip(self.weights, self.bias_weights):\n",
                "                bias_rounded = np.round(bias, 6)\n",
                "                weights_rounded = np.round(weights,6)\n",
                "                bias_reshaped = np.reshape(bias_rounded, (1, len(bias_rounded)))\n",
                "                integrated_layer_weights = np.vstack([bias_reshaped, weights_rounded])\n",
                "                model_data[\"final_weights\"].append(integrated_layer_weights.tolist())\n",
                "\n",
                "            new_file_name = \"model-\" + os.path.basename(file_name)\n",
                "            # Save to JSON file\n",
                "            with open(os.path.join(directory, new_file_name), \"w\") as json_file:\n",
                "                json.dump(model_data, json_file)\n",
                "            \n",
                "            print(\"Model saved successfully to JSON.\")\n",
                "\n",
                "    @classmethod\n",
                "    def load_model(cls, file_name, directory=\"model\"):\n",
                "        \"\"\"\n",
                "        Loads the model weights and configuration from model directory.\n",
                "        \"\"\"\n",
                "        # Load configuration\n",
                "        with open(os.path.join(directory, file_name), \"r\") as json_file:\n",
                "            model_data = json.load(json_file)\n",
                "        \n",
                "        layers = [FFNNLayer\n",
                "                  (layer_conf[\"number_of_neurons\"], layer_conf[\"activation_function\"])\n",
                "                  for layer_conf in model_data[\"config\"][\"layers\"]\n",
                "                ]\n",
                "        \n",
                "        #  Create new instance\n",
                "        classifier = cls(\n",
                "            layers=layers,\n",
                "            weights=[],  \n",
                "        )\n",
                "\n",
                "        classifier.weights = []\n",
                "        classifier.bias_weights = []\n",
                "        for integrated_weights in model_data[\"final_weights\"]:\n",
                "            np_weights = np.array(integrated_weights)\n",
                "            classifier.bias_weights.append(np_weights[0, :])\n",
                "            classifier.weights.append(np_weights[1:, :])  \n",
                "        return classifier\n",
                "    \n",
                "    def print_model(self):\n",
                "        for i in range(len(self.weights)):\n",
                "            print(\"[\")\n",
                "            print(\"  \", self.bias_weights[i])\n",
                "            for neuron_weight in self.weights[i]:\n",
                "                print(\"  \", neuron_weight)\n",
                "            print(\"], \")\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Main Program"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Dijalankan untuk melakukan pengujian berdasarkan test case yang diberikan. Test case diuji dengan menuliskan path dari file test case pada input."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "========= EXPECTED =========\n",
                        "[\n",
                        "   [0.1008, 0.3006, 0.1991]\n",
                        "   [0.402, 0.201, -0.7019]\n",
                        "   [0.101, -0.799, 0.4987]\n",
                        "], \n",
                        "STOPPED BY:  max_iteration\n",
                        "========== ACTUAL ==========\n",
                        "[\n",
                        "   [0.1012 0.3006 0.1991]\n",
                        "   [ 0.4024  0.201  -0.7019]\n",
                        "   [ 0.1018 -0.799   0.4987]\n",
                        "], \n",
                        "STOPPED BY:  max_iteration\n",
                        "Sum Squared Error: 0.0000\n",
                        "Sum Squared Error(SSE) of prediction surpass the Maximum SSE\n",
                        "Model saved successfully to JSON.\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "file_path = input(\"Enter json file path: \")\n",
                "f = open(file_path)\n",
                "data = json.load(f)\n",
                "\n",
                "try:\n",
                "    data_layers = data[\"case\"][\"model\"][\"layers\"]\n",
                "    layers = []\n",
                "    for layer in data_layers:\n",
                "        activation_func = layer[\"activation_function\"]\n",
                "        if activation_func not in [\"linear\", \"relu\", \"sigmoid\", \"softmax\"]:\n",
                "            raise Exception(\"Activation function \" + activation_func + \" not available\")\n",
                "        layers.append(FFNNLayer(layer[\"number_of_neurons\"], activation_func))\n",
                "\n",
                "    weights = data[\"case\"][\"initial_weights\"]\n",
                "    input_size = data[\"case\"][\"model\"][\"input_size\"]\n",
                "    X_train = data[\"case\"][\"input\"]\n",
                "    y_train = data[\"case\"][\"target\"]\n",
                "    learning_rate = data[\"case\"][\"learning_parameters\"][\"learning_rate\"]\n",
                "    batch_size = data[\"case\"][\"learning_parameters\"][\"batch_size\"]\n",
                "    max_iteration = data[\"case\"][\"learning_parameters\"][\"max_iteration\"]\n",
                "    error_threshold = data[\"case\"][\"learning_parameters\"][\"error_threshold\"]\n",
                "\n",
                "    expected_weights = data[\"expect\"][\"final_weights\"]\n",
                "    expected_stopped_by = data[\"expect\"][\"stopped_by\"]\n",
                "\n",
                "    model = MLPClassifier(layers, weights, learning_rate, error_threshold, max_iteration, batch_size, expected_stopped_by, expected_weights)\n",
                "\n",
                "    model.fit(X_train, y_train)\n",
                "\n",
                "    sse = model.calculate_sse()\n",
                "    print(f\"Sum Squared Error: {sse:.4f}\")\n",
                "    if sse < 1e-7:\n",
                "        print(\"Sum Squared Error(SSE) of prediction is lower than Maximum SSE\")\n",
                "    else:\n",
                "        print(\"Sum Squared Error(SSE) of prediction surpass the Maximum SSE\")\n",
                "    model.save_model(file_path)\n",
                "except KeyError as ke:\n",
                "    print('Key', ke, \"not found in json data. Please check your json data format\")\n",
                "except Exception as error:\n",
                "    print(\"An exception occurred: \", error)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Pengujian test case menggunakan library **Keras Tensorflow**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.2217\n",
                        "========= EXPECTED =========\n",
                        "[\n",
                        "   [0.1008, 0.3006, 0.1991]\n",
                        "   [0.402, 0.201, -0.7019]\n",
                        "   [0.101, -0.799, 0.4987]\n",
                        "], \n",
                        "STOPPED BY:  max_iteration\n",
                        "========== ACTUAL ==========\n",
                        "[\n",
                        "   [0.1004, 0.30020002, 0.1997]\n",
                        "   [ 0.40080002  0.20033334 -0.70063335]\n",
                        "   [ 0.1006     -0.7996667   0.49956667]\n",
                        "], \n"
                    ]
                }
            ],
            "source": [
                "import tensorflow as tf\n",
                "\n",
                "model = tf.keras.Sequential()\n",
                "model.add(tf.keras.Input(shape=(input_size,)))\n",
                "\n",
                "initial_weights = [np.array(layer) for layer in weights]\n",
                "\n",
                "for i, layer_data in enumerate(data_layers):\n",
                "  layer = tf.keras.layers.Dense(units=layer_data[\"number_of_neurons\"], activation=layer_data[\"activation_function\"], use_bias=True)\n",
                "\n",
                "  if i == 0:\n",
                "    layer.build(input_shape=(input_size,))\n",
                "  else:\n",
                "    layer.build(input_shape=(data_layers[i-1][\"number_of_neurons\"],))\n",
                "  \n",
                "  layer_bias = initial_weights[i][0]\n",
                "  layer_weight = np.array(initial_weights[i][1:])\n",
                "  layer.set_weights([layer_weight, layer_bias])\n",
                "  model.add(layer)\n",
                "\n",
                "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate), loss='mean_squared_error')\n",
                "model.fit(np.array(X_train), np.array(y_train), epochs=max_iteration, batch_size=batch_size)\n",
                "\n",
                "final_weights = []\n",
                "for i, layer in enumerate(model.layers):\n",
                "  keras_weights, biases = layer.get_weights()\n",
                "  combined_layer_weights = [list(biases)] + list(keras_weights)\n",
                "  final_weights.append(combined_layer_weights)\n",
                "\n",
                "print(\"========= EXPECTED =========\")\n",
                "for weight in expected_weights:\n",
                "    print(\"[\")\n",
                "    for neuron_weight in weight:\n",
                "        print(\"  \", neuron_weight)\n",
                "    print(\"], \")\n",
                "print(\"STOPPED BY: \", expected_stopped_by)\n",
                "print(\"========== ACTUAL ==========\")\n",
                "for weight in final_weights:\n",
                "    print(\"[\")\n",
                "    for neuron_weight in weight:\n",
                "        print(\"  \", neuron_weight)\n",
                "    print(\"], \")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Pengujian pada Dataset Iris"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Akan dilakukan pengujian pada dataset iris dengan parameter sebagai berikut:\n",
                "* Struktur jaringan: 1 hidden layer 4 neuron dan fungsi aktivasi ReLU\n",
                "* Initial weights: akan diinitialize secara random dengan nilai dalam interval -0.5 - 0.5\n",
                "* learning_rate: 0.1\n",
                "* error_threshold: 0.0001\n",
                "* max_iter: 1000\n",
                "* batch_size: 50"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Preprocessing data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Load the data\n",
                "data = pd.read_csv(\"test_cases_mlp/iris.csv\")\n",
                "\n",
                "# Split the data\n",
                "X = data.drop(columns=[\"Species\", \"Id\"])\n",
                "y = data[\"Species\"]\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
                "\n",
                "# Standardize the data\n",
                "scaler = StandardScaler()\n",
                "X_train = scaler.fit_transform(X_train)\n",
                "X_test = scaler.transform(X_test)\n",
                "\n",
                "# Encode the data\n",
                "encoder = LabelEncoder()\n",
                "y_train_encoded = encoder.fit_transform(y_train)\n",
                "y_test_encoded = encoder.transform(y_test)\n",
                "\n",
                "def one_hot_encode(labels, num_classes):\n",
                "    one_hot = np.zeros((len(labels), num_classes), dtype=int)\n",
                "    one_hot[np.arange(len(labels)), labels] = 1\n",
                "    return one_hot.tolist()\n",
                "\n",
                "# One-hot encode the labels\n",
                "y_train = one_hot_encode(y_train_encoded, 3)\n",
                "y_test = one_hot_encode(y_test_encoded, 3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Pengujian pada kelas implementasi MLPClassifier"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Accuracy Score:  0.9666666666666667\n",
                        "Model saved successfully to JSON.\n",
                        "[\n",
                        "   [-0.385767  0.707035 -2.181451  8.43606 ]\n",
                        "   [-0.197066 -1.517243 -0.265902  1.440485]\n",
                        "   [-0.060373  1.624506  0.782409  2.369535]\n",
                        "   [-0.060027 -2.781527 -0.14648  -7.446527]\n",
                        "   [-0.021666 -2.624976  0.328015 -4.714587]\n",
                        "], \n",
                        "[\n",
                        "   [-5.005029  1.090531  4.0993  ]\n",
                        "   [-0.137552 -0.105708  0.065956]\n",
                        "   [ 3.41615  -2.726317 -0.84273 ]\n",
                        "   [-0.524307  0.482128  0.833498]\n",
                        "   [ 2.080643  3.807932 -5.52697 ]\n",
                        "], \n",
                        "Accuracy Score by loaded model:  0.9666666666666667\n"
                    ]
                }
            ],
            "source": [
                "import random\n",
                "\n",
                "# Softmax layer is added because the iris dataset is a classification task, aligned with sklearn implementation\n",
                "layers = [\n",
                "    FFNNLayer(4, 'relu'),\n",
                "    FFNNLayer(3, 'softmax')\n",
                "]\n",
                "\n",
                "iteration = 0\n",
                "max_score, max_model = -1, None\n",
                "while iteration < 100:\n",
                "    # Generate random weights for each layer\n",
                "    initial_weights = []\n",
                "    for i, layer in enumerate(layers):\n",
                "        if i == 0:\n",
                "            num_rows = len(X_train[0]) + 1\n",
                "        else:\n",
                "            num_rows = layers[i - 1].number_of_neurons + 1\n",
                "        \n",
                "        layer_weights = [[random.uniform(-0.5, 0.5) for _ in range(layer.number_of_neurons)] for _ in range(num_rows)]\n",
                "        initial_weights.append(layer_weights)\n",
                "\n",
                "    implementation_model = MLPClassifier(layers=layers, weights=initial_weights, learning_rate=0.1, error_threshold=0.0001, max_iter=1000, batch_size=50, stopped_by=\"error_threshold\")\n",
                "    implementation_model.fit(X_train, y_train)\n",
                "    prediction_implementation_model = implementation_model.predict(X_test)\n",
                "\n",
                "    score = implementation_model.calc_score(y_test, prediction_implementation_model)\n",
                "\n",
                "    if score > max_score:\n",
                "        max_score = score\n",
                "        max_model = implementation_model\n",
                "\n",
                "    if max_score > 0.95:\n",
                "        break\n",
                "\n",
                "    iteration += 1\n",
                "\n",
                "# Save the model\n",
                "print(\"Accuracy Score: \", max_score)\n",
                "max_model.save_model(\"iris.json\")\n",
                "\n",
                "# Load the model again to check\n",
                "loaded_model = MLPClassifier.load_model(\"model-iris.json\")\n",
                "loaded_model.print_model()\n",
                "\n",
                "prediction_with_loaded_model = loaded_model.predict(X_test)\n",
                "score_loaded_model = loaded_model.calc_score(y_test, prediction_with_loaded_model)\n",
                "print(\"Accuracy Score by loaded model: \", score_loaded_model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Pengujian menggunakan library scikit-learn"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Dikarenakan inisialisasi model pada library scikit-learn tidak dapat didefinisikan initial weights-nya, maka parameter initial_weights tidak digunakan."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Accuracy Score:  1.0\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.neural_network import MLPClassifier\n",
                "# Define the model with mini-batch gradient descent \n",
                "model = MLPClassifier(hidden_layer_sizes=(4),learning_rate='constant', learning_rate_init=0.1, alpha=0.0001, solver='sgd', batch_size=50, max_iter=1000, activation='relu') \n",
                "  \n",
                "# Train model\n",
                "model.fit(X_train, y_train) \n",
                "  \n",
                "# Evaluate model\n",
                "score = model.score(X_test, y_test) \n",
                "print(\"Accuracy Score: \", score) "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
