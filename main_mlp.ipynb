{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPClassifier Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter yang ada pada kelas MLPClassifier yaitu: **struktur jaringan (jumlah layer, jumlah neuron setiap layer, fungsi aktivasi setiap layer), initial weights tiap neuron, learning_rate, error_threshold, max_iter, batch_size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class FFNNLayer:\n",
    "    def __init__(self, number_of_neurons: int, activation_function: str):\n",
    "        \"\"\"\n",
    "        :param number_of_neurons:\n",
    "        :param activation_function:\n",
    "        \"\"\"\n",
    "        self.number_of_neurons = number_of_neurons\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "\n",
    "class MLPClassifier:\n",
    "    def __init__(self, layers: list, learning_rate, error_threshold, max_iter, batch_size, weights, stopped_by, expected_weights = None):\n",
    "        \"\"\"\n",
    "        :param layers: list of FFNNLayer to specify the activation function and num of neurons for each layers\n",
    "        :param learning_rate: the learning rate\n",
    "        :param error_threshold: the error threshold\n",
    "        :param max_iter: max iter to stop iteration\n",
    "        :param batch_size: the size of batch for each mini batch\n",
    "        \"\"\"\n",
    "        self.num_of_layers = len(layers)\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.error_threshold = error_threshold\n",
    "        self.error_sum = 1\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.X_train = []\n",
    "        self.y_train = []\n",
    "        self.neuron_values = []\n",
    "        self.weights = [weight[1:] for weight in weights]\n",
    "        self.bias_weights = [weight[0] for weight in weights]\n",
    "        self.prediction = []\n",
    "        self.num_of_features = 0\n",
    "        self.num_of_batches = 0\n",
    "        self.d_weights = None\n",
    "        self.d_bias_weights = None\n",
    "        self.expected_stopped_by = stopped_by\n",
    "        self.expected_weights = expected_weights\n",
    "        self.expected_output = None\n",
    "        self.stopped_by = None\n",
    "        self.current_inputs = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.num_of_features = len(self.X_train)\n",
    "        self.num_of_batches = math.ceil(len(self.X_train) / self.batch_size)\n",
    "\n",
    "        # the first neuron is the X inputs themselves\n",
    "        self.neuron_values = [[None for _ in range(layer.number_of_neurons)] for layer in self.layers]\n",
    "        num_iter = 0\n",
    "        while num_iter < self.max_iter:\n",
    "            num_of_batches = math.ceil(len(self.X_train) / self.batch_size)\n",
    "            err = 0\n",
    "            for i in range(num_of_batches):\n",
    "                self.__forward(i)\n",
    "                self.__backward(i)\n",
    "                err += self.__calculate_error(i)\n",
    "\n",
    "            # Update the average error for this iteration\n",
    "            self.error_sum = err / num_of_batches\n",
    "\n",
    "            # Check if the error is below the threshold\n",
    "            if self.error_sum <= self.error_threshold:\n",
    "                break\n",
    "\n",
    "            num_iter += 1\n",
    "\n",
    "        self.stopped_by = \"max_iteration\" if num_iter == self.max_iter else \"error_threshold\"\n",
    "\n",
    "        if self.expected_weights:\n",
    "            self.__print_final_weights()\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Perform forward pass to make predictions on input X_test\n",
    "\n",
    "        Args:\n",
    "            X_test: Input data for prediction (list)\n",
    "\n",
    "        Returns:\n",
    "            Predicted outputs for each sample in X_test\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        current_inputs = np.array(X_test)\n",
    "        for i in range(self.num_of_layers):\n",
    "            net = np.matmul(current_inputs, self.weights[i]) + self.bias_weights[i]\n",
    "            act_func = self.layers[i].activation_function\n",
    "            if act_func == 'linear':\n",
    "                res = [linear(x) for x in net]\n",
    "            elif act_func == 'relu':\n",
    "                res = [relu(n) for n in net]\n",
    "            elif act_func == 'sigmoid':\n",
    "                res = [sigmoid(n) for n in net]\n",
    "            elif act_func == \"softmax\":\n",
    "                res = [softmax(n) for n in net]\n",
    "            current_inputs = res\n",
    "        predictions = res.toList()\n",
    "        return predictions\n",
    "\n",
    "    def calculate_sse(self):\n",
    "        sse = 0\n",
    "        for layer in range(len(self.expected_weights)):\n",
    "            for neuron in range(len(self.expected_weights[layer])):\n",
    "                expected = np.array(self.expected_weights[layer][neuron])\n",
    "                result = self.bias_weights[layer] if neuron == 0 else self.weights[layer][neuron-1]\n",
    "                squared_error = (expected - result) ** 2\n",
    "                sse += np.sum(squared_error)\n",
    "        return sse\n",
    "\n",
    "    def __forward(self, batch):\n",
    "        start_idx = self.batch_size * batch\n",
    "        self.expected_output = self.y_train[start_idx:start_idx + self.__get_curr_batch_size(batch)]\n",
    "        self.current_inputs = self.X_train[start_idx:start_idx + self.__get_curr_batch_size(batch)]\n",
    "        res = self.current_inputs\n",
    "        for i in range(self.num_of_layers):\n",
    "            net = [np.matmul(x, self.weights[i]) + self.bias_weights[i] for x in res]\n",
    "            act_func = self.layers[i].activation_function\n",
    "            if act_func == 'linear':\n",
    "                res = [linear(x) for x in net]\n",
    "            if act_func == 'relu':\n",
    "                res = [relu(n) for n in net]\n",
    "            if act_func == 'sigmoid':\n",
    "                res = [sigmoid(n) for n in net]\n",
    "            if act_func == \"softmax\":\n",
    "                res = [softmax(n) for n in net]\n",
    "            self.neuron_values[i] = res\n",
    "        self.prediction = list(self.neuron_values[-1])\n",
    "\n",
    "    def __backward(self, batch_idx):\n",
    "        \"\"\"\n",
    "        do backward propagation for each batch\n",
    "        :param batch_idx: the current batch that is processed\n",
    "        \"\"\"\n",
    "        self.__init_d_weights()\n",
    "        # get the current batch size\n",
    "        batch_size = self.__get_curr_batch_size(batch_idx)\n",
    "\n",
    "        # for each X in the batch\n",
    "        for i in range(batch_size):\n",
    "            d_k = np.zeros(0)\n",
    "            for j in range(self.num_of_layers - 1, -1, -1):\n",
    "                if j == self.num_of_layers - 1:\n",
    "                    delta = self.__calc_output_layer_delta(i)\n",
    "                else:\n",
    "                    delta = self.__calc_hidden_layer_delta(i, j, d_k)\n",
    "                x = self.current_inputs[i] if j == 0 else self.neuron_values[j - 1][i]\n",
    "                self.d_weights[j] += np.array([[d * n for d in delta] for n in x])\n",
    "                self.d_bias_weights[j] += np.array(delta)\n",
    "                d_k = delta.reshape(delta.shape[0], 1)\n",
    "        \n",
    "        self.weights = [np.array(self.weights[k]) + np.array(self.d_weights[k]) * self.learning_rate for k in\n",
    "                        range(len(self.weights))]\n",
    "        self.bias_weights = [np.array(self.bias_weights[k]) + np.array(self.d_bias_weights[k]) * self.learning_rate for\n",
    "                             k in range(len(self.bias_weights))]\n",
    "\n",
    "    def __calculate_error(self, batch_idx):\n",
    "        \"\"\"\n",
    "        Calculate the error for the current batch\n",
    "        :param batch_idx: the current batch that is processed\n",
    "        \"\"\"\n",
    "        start_idx = self.batch_size * batch_idx\n",
    "        end_idx = start_idx + self.__get_curr_batch_size(batch_idx)\n",
    "        y_true = np.array(self.y_train[start_idx:end_idx])\n",
    "        y_pred = np.array(self.prediction)\n",
    "\n",
    "        # Get the activation function of the output layer\n",
    "        act_func = self.layers[-1].activation_function\n",
    "\n",
    "        # Calculate the error based on the activation function\n",
    "        if act_func in ['relu', 'sigmoid', 'linear']:\n",
    "            return 0.5 * np.sum((y_true - y_pred) ** 2)\n",
    "        elif act_func == 'softmax':\n",
    "            return -np.sum(y_true * np.log(y_pred))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {act_func}\")\n",
    "\n",
    "    def __update_weights(self):\n",
    "        self.weights = [np.array(self.weights[k]) + np.array(self.d_weights[k]) * self.learning_rate for k in\n",
    "                        range(len(self.weights))]\n",
    "        self.bias_weights = [np.array(self.bias_weights[k]) + np.array(self.d_bias_weights[k]) * self.learning_rate for\n",
    "                             k in range(len(self.bias_weights))]\n",
    "\n",
    "    def __init_d_weights(self):\n",
    "        self.d_weights = [np.array([np.zeros(len(neuron_weight)) for neuron_weight in layer_weight])\n",
    "                          for layer_weight in self.weights]\n",
    "        self.d_bias_weights = [np.zeros(layer.number_of_neurons) for layer in self.layers]\n",
    "\n",
    "    def __calc_output_diff(self, x_idx: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param x_idx:  the index of the current input on the X_train\n",
    "        \"\"\"\n",
    "        y_train = self.expected_output[x_idx]  # get the expected output of the x\n",
    "        output = self.prediction[x_idx]  # get the prediction\n",
    "        return np.array([y - p for y, p in zip(y_train, output)])\n",
    "\n",
    "    def __calc_act_function_derivative(self, act_func: str, y: list, target=None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param y:  y is the output in a layer\n",
    "\n",
    "        :return : a 1D array which is the sigmoid gradient of the neurons in a layer\n",
    "        \"\"\"\n",
    "        if act_func == 'sigmoid':\n",
    "            return np.array([x * (1-x) for x in y])\n",
    "\n",
    "        elif act_func == 'relu':\n",
    "            return np.array([1 if x > 0 else 0 for x in y])\n",
    "\n",
    "        elif act_func == 'linear':\n",
    "            return np.array([1 for _ in y])\n",
    "\n",
    "        elif act_func == 'softmax':\n",
    "            if target is None:\n",
    "                raise ValueError(\"Target is required for softmax gradient\")\n",
    "            return np.array([-1 * (1-y[i]) if target == i else y[i] for i in range(len(y))])\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function: {act_func}\")\n",
    "\n",
    "\n",
    "    def __calc_output_layer_delta(self, x_idx: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param x_idx:  the index of the current input on the X_train\n",
    "        \"\"\"\n",
    "        # get the activation function for the last layer (output layer)\n",
    "        act_func = self.layers[-1].activation_function  # get the activation function\n",
    "        if act_func == 'softmax':\n",
    "            return self.__calc_output_diff(x_idx)\n",
    "        return self.__calc_act_function_derivative(act_func, self.prediction[x_idx]) * self.__calc_output_diff(x_idx)\n",
    "\n",
    "    def __calc_hidden_layer_delta(self, batch_idx, layer_idx: int, output_error_term: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param output_error_term: a 1D array of the error term of each weight calculated from the layer after\n",
    "        :param layer_idx: the index of the current layer\n",
    "        :param batch_idx: the index of the current batch\n",
    "\n",
    "        hidden layer gradient = net gradient of the neuron values of current layer * the sum of weight * output error term\n",
    "        \"\"\"\n",
    "        act_func = self.layers[layer_idx].activation_function\n",
    "        activation_func_derivative = self.__calc_act_function_derivative(act_func,\n",
    "                                                                         self.neuron_values[layer_idx][batch_idx])\n",
    "\n",
    "        sum_d_net = [x[0] for x in np.matmul(self.weights[layer_idx + 1], output_error_term)]\n",
    "        return np.array(activation_func_derivative\n",
    "                        * sum_d_net)\n",
    "\n",
    "    def __get_curr_batch_size(self, batch_idx):\n",
    "        mod_res = len(self.X_train) % self.batch_size\n",
    "        if batch_idx == self.batch_size - 1 and mod_res != 0:\n",
    "            return mod_res\n",
    "        return self.batch_size\n",
    "\n",
    "    def __print_final_weights(self):\n",
    "        print(\"========= EXPECTED =========\")\n",
    "        for weight in self.expected_weights:\n",
    "            print(\"[\")\n",
    "            for neuron_weight in weight:\n",
    "                print(\"  \", neuron_weight)\n",
    "            print(\"], \")\n",
    "        print(\"STOPPED BY: \", self.expected_stopped_by)\n",
    "\n",
    "        print(\"========== ACTUAL ==========\")\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            print(\"[\")\n",
    "            print(\"  \", self.bias_weights[i])\n",
    "            for neuron_weight in self.weights[i]:\n",
    "                print(\"  \", neuron_weight)\n",
    "            print(\"], \")\n",
    "        print(\"STOPPED BY: \", self.stopped_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dijalankan untuk melakukan pengujian berdasarkan test case yang diberikan. Test case diuji dengan menuliskan path dari file test case pada input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= EXPECTED =========\n",
      "[\n",
      "   [-0.28730211, -0.28822282, -0.70597451, 0.42094471]\n",
      "   [-0.5790794, -1.1836444, -1.34287961, 0.69575311]\n",
      "   [-0.41434377, 1.51314676, -0.97649086, -1.3043465]\n",
      "], \n",
      "[\n",
      "   [-1.72078607, 1.74078607]\n",
      "   [-0.50352956, 0.48352956]\n",
      "   [1.25764816, -1.23764816]\n",
      "   [-1.16998784, 1.14998784]\n",
      "   [1.0907634, -1.0707634]\n",
      "], \n",
      "STOPPED BY:  error_threshold\n",
      "========== ACTUAL ==========\n",
      "[\n",
      "   [-0.28730211 -0.28822282 -0.70597451  0.42094471]\n",
      "   [-0.5790794  -1.1836444  -1.34287961  0.69575311]\n",
      "   [-0.41434377  1.51314676 -0.97649086 -1.3043465 ]\n",
      "], \n",
      "[\n",
      "   [-1.72078607  1.74078607]\n",
      "   [-0.50352956  0.48352956]\n",
      "   [ 1.25764816 -1.23764816]\n",
      "   [-1.16998784  1.14998784]\n",
      "   [ 1.0907634 -1.0707634]\n",
      "], \n",
      "STOPPED BY:  error_threshold\n",
      "Sum Squared Error: 0.0000\n",
      "Sum Squared Error(SSE) of prediction is lower than Maximum SSE\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = input(\"Enter json file path: \")\n",
    "f = open(file_path)\n",
    "data = json.load(f)\n",
    "\n",
    "try:\n",
    "    data_layers = data[\"case\"][\"model\"][\"layers\"]\n",
    "    layers = []\n",
    "    for layer in data_layers:\n",
    "        activation_func = layer[\"activation_function\"]\n",
    "        if activation_func not in [\"linear\", \"relu\", \"sigmoid\", \"softmax\"]:\n",
    "            raise Exception(\"Activation function \" + activation_func + \" not available\")\n",
    "        layers.append(FFNNLayer(layer[\"number_of_neurons\"], activation_func))\n",
    "\n",
    "    weights = data[\"case\"][\"initial_weights\"]\n",
    "    input_size = data[\"case\"][\"model\"][\"input_size\"]\n",
    "    X_train = data[\"case\"][\"input\"]\n",
    "    y_train = data[\"case\"][\"target\"]\n",
    "    learning_rate = data[\"case\"][\"learning_parameters\"][\"learning_rate\"]\n",
    "    batch_size = data[\"case\"][\"learning_parameters\"][\"batch_size\"]\n",
    "    max_iteration = data[\"case\"][\"learning_parameters\"][\"max_iteration\"]\n",
    "    error_threshold = data[\"case\"][\"learning_parameters\"][\"error_threshold\"]\n",
    "\n",
    "    expected_weights = data[\"expect\"][\"final_weights\"]\n",
    "    expected_stopped_by = data[\"expect\"][\"stopped_by\"]\n",
    "\n",
    "    model = MLPClassifier(layers, learning_rate, error_threshold, max_iteration, batch_size, weights, expected_stopped_by, expected_weights)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    sse = model.calculate_sse()\n",
    "    print(f\"Sum Squared Error: {sse:.4f}\")\n",
    "    if sse < 1e-7:\n",
    "        print(\"Sum Squared Error(SSE) of prediction is lower than Maximum SSE\")\n",
    "    else:\n",
    "        print(\"Sum Squared Error(SSE) of prediction surpass the Maximum SSE\")\n",
    "except KeyError as ke:\n",
    "    print('Key', ke, \"not found in json data. Please check your json data format\")\n",
    "except Exception as error:\n",
    "    print(\"An exception occurred: \", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pengujian pada Dataset Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada dataset iris, perlu dilakukan preprocessing sehingga dibuat kelas berikut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, file_path: str = None):\n",
    "        self.data = None\n",
    "        self.encoder = LabelEncoder()\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "        if file_path is not None:\n",
    "            self.load_data(file_path)\n",
    "\n",
    "    def load_data(self, file_path: str):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "\n",
    "    def preprocess(self, target_column: str):\n",
    "        \"\"\"\n",
    "        Preprocesses the data by splitting it, standardizing the features, and encoding the labels.\n",
    "\n",
    "        :param target_column: The name of the column to use as the target (label).\n",
    "\n",
    "        :return: The standardized features and encoded labels for the training and testing data.\n",
    "        \"\"\"\n",
    "        X_train, X_test, y_train, y_test = self.__split_data(0.2, target_column)\n",
    "        X_train_standardized, X_test_standardized = self.__standardize_data(X_train, X_test)\n",
    "        y_train_encoded, y_test_encoded = self.__encode_data(y_train, y_test)\n",
    "\n",
    "        return X_train_standardized, X_test_standardized, y_train_encoded, y_test_encoded\n",
    "\n",
    "    def __split_data(self, test_size: float = 0.2, stratify_column_label: str = None):\n",
    "        \"\"\"\n",
    "        Splits the data into training and testing sets.\n",
    "\n",
    "        :param test_size: The proportion of the dataset to include in the test split.\n",
    "        :param stratify_column_label: The name of the column to use for stratification.\n",
    "\n",
    "        :return: The training and testing data and labels.\n",
    "        \"\"\"\n",
    "        X = self.data.drop(columns=[stratify_column_label])\n",
    "        y = self.data[stratify_column_label]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,\n",
    "                                                            stratify=y if stratify_column_label else None)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def __standardize_data(self, X_train, X_test):\n",
    "        \"\"\"\n",
    "        Standardizes the features in the training and testing data.\n",
    "\n",
    "        :param X_train: The training data.\n",
    "        :param X_test: The testing data.\n",
    "\n",
    "        :return: The standardized training and testing data.\n",
    "        \"\"\"\n",
    "        X_train_standardized = self.scaler.fit_transform(X_train)\n",
    "        X_test_standardized = self.scaler.transform(X_test)\n",
    "\n",
    "        return X_train_standardized, X_test_standardized\n",
    "\n",
    "    def __encode_data(self, y_train, y_test):\n",
    "        \"\"\"\n",
    "        Encodes the labels in the training and testing data.\n",
    "\n",
    "        :param y_train: The labels for the training data.\n",
    "        :param y_test: The labels for the testing data.\n",
    "\n",
    "        :return: The encoded labels for the training and testing data.\n",
    "        \"\"\"\n",
    "        y_train_encoded = self.encoder.fit_transform(y_train)\n",
    "        y_test_encoded = self.encoder.transform(y_test)\n",
    "\n",
    "        return y_train_encoded, y_test_encoded\n",
    "\n",
    "    def decode_labels(self, y_encoded: list):\n",
    "        \"\"\"\n",
    "        Decodes the encoded labels back to the original categorical labels.\n",
    "\n",
    "        :param y_encoded: The encoded labels.\n",
    "\n",
    "        :return: The original categorical labels.\n",
    "        \"\"\"\n",
    "        y_original = self.encoder.inverse_transform(y_encoded)\n",
    "        return y_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Akan dilakukan pengujian pada dataset iris dengan parameter sebagai berikut:\n",
    "* Struktur jaringan: 2 hidden layer dengan masing-masing 3 neuron dan fungsi aktivasi ReLU\n",
    "* Initial weights: akan diinitialize secara random dengan nilai dalam interval -0.5 - 0.5\n",
    "* learning_rate: 0.1\n",
    "* error_threshold: 0.0001\n",
    "* max_iter: 100\n",
    "* batch_size: 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset \n",
    "preprocessor = DataPreprocessor(\"test_cases_mlp/iris.csv\")\n",
    "\n",
    "# Split the data into training and test sets \n",
    "X_train, X_test, y_train, y_test = preprocessor.preprocess(\"Species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pengujian pada kelas implementasi MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pengujian menggunakan library scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
